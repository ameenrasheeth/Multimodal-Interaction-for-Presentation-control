# Multimodal Interaction for Presentation Control
This project enables controlling presentations using multiple input methods such as hand gestures and voice commands. It is built with Python and leverages computer vision and audio processing techniques.

## Features
Hand Gesture Recognition: Use predefined hand gestures to control presentation slides.
Voice Commands: Navigate through slides using voice commands.

## Installation
### Clone the repository:

git clone https://github.com/ShakthiUdhai/Multimodal-Interaction-for-presentation-control.git
cd Multimodal-Interaction-for-presentation-control

### Install the required dependencies:

pip install -r requirements.txt

### Link to download the vosk model

https://alphacephei.com/vosk/models/vosk-model-en-in-0.5.zip

## Usage
### Run the main application:

python app.py

## File Descriptions
- HandGesture.py: Contains the code for hand gesture recognition.
- audio.py: Manages audio input and processes voice commands.
- app.py: Main application script that integrates hand gestures and voice commands for controlling presentations.

## Contributing
Contributions are welcome! Please fork the repository and submit a pull request for any enhancements or bug fixes.

## Contact
For any questions or feedback, please contact udhaishakthi523@gmail.com
